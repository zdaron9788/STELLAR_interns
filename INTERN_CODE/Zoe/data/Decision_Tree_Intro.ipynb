{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Tree Classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "---\n",
    "**Supervised machine learning** tries to learn patterns in data to make predictions for new data. **Decision trees** are one algorithm for doing this. Fancier models exist, which give more accurate predictions, but decision trees are easy to understand, and they are the basic building block for some of the best models in data science. Through the guidance of this notebook, you will learn how to train and evaluate a decision tree and predict future data. \n",
    " \n",
    "Before we jump into using a dataset from the astronomical literature, let's start with the [Iris dataset](https://en.wikipedia.org/wiki/Iris_flower_data_set) to get a feel for how to build and evaluate a decision tree. If you are coming across this for the first time, you are highly encouraged to visit the hyperlink above and understand what this data represents. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['setosa', 'versicolor', 'virginica']"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.datasets import load_iris\n",
    "#if above gives errors, open a terminal window and type: pip install sklearn\n",
    "\n",
    "data = load_iris() #a preloaded dataset from sklearn\n",
    "list(data.target_names) #different types of iris flowers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': array([[5.1, 3.5, 1.4, 0.2],\n",
       "        [4.9, 3. , 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.3, 0.2],\n",
       "        [4.6, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.6, 1.4, 0.2],\n",
       "        [5.4, 3.9, 1.7, 0.4],\n",
       "        [4.6, 3.4, 1.4, 0.3],\n",
       "        [5. , 3.4, 1.5, 0.2],\n",
       "        [4.4, 2.9, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.1],\n",
       "        [5.4, 3.7, 1.5, 0.2],\n",
       "        [4.8, 3.4, 1.6, 0.2],\n",
       "        [4.8, 3. , 1.4, 0.1],\n",
       "        [4.3, 3. , 1.1, 0.1],\n",
       "        [5.8, 4. , 1.2, 0.2],\n",
       "        [5.7, 4.4, 1.5, 0.4],\n",
       "        [5.4, 3.9, 1.3, 0.4],\n",
       "        [5.1, 3.5, 1.4, 0.3],\n",
       "        [5.7, 3.8, 1.7, 0.3],\n",
       "        [5.1, 3.8, 1.5, 0.3],\n",
       "        [5.4, 3.4, 1.7, 0.2],\n",
       "        [5.1, 3.7, 1.5, 0.4],\n",
       "        [4.6, 3.6, 1. , 0.2],\n",
       "        [5.1, 3.3, 1.7, 0.5],\n",
       "        [4.8, 3.4, 1.9, 0.2],\n",
       "        [5. , 3. , 1.6, 0.2],\n",
       "        [5. , 3.4, 1.6, 0.4],\n",
       "        [5.2, 3.5, 1.5, 0.2],\n",
       "        [5.2, 3.4, 1.4, 0.2],\n",
       "        [4.7, 3.2, 1.6, 0.2],\n",
       "        [4.8, 3.1, 1.6, 0.2],\n",
       "        [5.4, 3.4, 1.5, 0.4],\n",
       "        [5.2, 4.1, 1.5, 0.1],\n",
       "        [5.5, 4.2, 1.4, 0.2],\n",
       "        [4.9, 3.1, 1.5, 0.2],\n",
       "        [5. , 3.2, 1.2, 0.2],\n",
       "        [5.5, 3.5, 1.3, 0.2],\n",
       "        [4.9, 3.6, 1.4, 0.1],\n",
       "        [4.4, 3. , 1.3, 0.2],\n",
       "        [5.1, 3.4, 1.5, 0.2],\n",
       "        [5. , 3.5, 1.3, 0.3],\n",
       "        [4.5, 2.3, 1.3, 0.3],\n",
       "        [4.4, 3.2, 1.3, 0.2],\n",
       "        [5. , 3.5, 1.6, 0.6],\n",
       "        [5.1, 3.8, 1.9, 0.4],\n",
       "        [4.8, 3. , 1.4, 0.3],\n",
       "        [5.1, 3.8, 1.6, 0.2],\n",
       "        [4.6, 3.2, 1.4, 0.2],\n",
       "        [5.3, 3.7, 1.5, 0.2],\n",
       "        [5. , 3.3, 1.4, 0.2],\n",
       "        [7. , 3.2, 4.7, 1.4],\n",
       "        [6.4, 3.2, 4.5, 1.5],\n",
       "        [6.9, 3.1, 4.9, 1.5],\n",
       "        [5.5, 2.3, 4. , 1.3],\n",
       "        [6.5, 2.8, 4.6, 1.5],\n",
       "        [5.7, 2.8, 4.5, 1.3],\n",
       "        [6.3, 3.3, 4.7, 1.6],\n",
       "        [4.9, 2.4, 3.3, 1. ],\n",
       "        [6.6, 2.9, 4.6, 1.3],\n",
       "        [5.2, 2.7, 3.9, 1.4],\n",
       "        [5. , 2. , 3.5, 1. ],\n",
       "        [5.9, 3. , 4.2, 1.5],\n",
       "        [6. , 2.2, 4. , 1. ],\n",
       "        [6.1, 2.9, 4.7, 1.4],\n",
       "        [5.6, 2.9, 3.6, 1.3],\n",
       "        [6.7, 3.1, 4.4, 1.4],\n",
       "        [5.6, 3. , 4.5, 1.5],\n",
       "        [5.8, 2.7, 4.1, 1. ],\n",
       "        [6.2, 2.2, 4.5, 1.5],\n",
       "        [5.6, 2.5, 3.9, 1.1],\n",
       "        [5.9, 3.2, 4.8, 1.8],\n",
       "        [6.1, 2.8, 4. , 1.3],\n",
       "        [6.3, 2.5, 4.9, 1.5],\n",
       "        [6.1, 2.8, 4.7, 1.2],\n",
       "        [6.4, 2.9, 4.3, 1.3],\n",
       "        [6.6, 3. , 4.4, 1.4],\n",
       "        [6.8, 2.8, 4.8, 1.4],\n",
       "        [6.7, 3. , 5. , 1.7],\n",
       "        [6. , 2.9, 4.5, 1.5],\n",
       "        [5.7, 2.6, 3.5, 1. ],\n",
       "        [5.5, 2.4, 3.8, 1.1],\n",
       "        [5.5, 2.4, 3.7, 1. ],\n",
       "        [5.8, 2.7, 3.9, 1.2],\n",
       "        [6. , 2.7, 5.1, 1.6],\n",
       "        [5.4, 3. , 4.5, 1.5],\n",
       "        [6. , 3.4, 4.5, 1.6],\n",
       "        [6.7, 3.1, 4.7, 1.5],\n",
       "        [6.3, 2.3, 4.4, 1.3],\n",
       "        [5.6, 3. , 4.1, 1.3],\n",
       "        [5.5, 2.5, 4. , 1.3],\n",
       "        [5.5, 2.6, 4.4, 1.2],\n",
       "        [6.1, 3. , 4.6, 1.4],\n",
       "        [5.8, 2.6, 4. , 1.2],\n",
       "        [5. , 2.3, 3.3, 1. ],\n",
       "        [5.6, 2.7, 4.2, 1.3],\n",
       "        [5.7, 3. , 4.2, 1.2],\n",
       "        [5.7, 2.9, 4.2, 1.3],\n",
       "        [6.2, 2.9, 4.3, 1.3],\n",
       "        [5.1, 2.5, 3. , 1.1],\n",
       "        [5.7, 2.8, 4.1, 1.3],\n",
       "        [6.3, 3.3, 6. , 2.5],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [7.1, 3. , 5.9, 2.1],\n",
       "        [6.3, 2.9, 5.6, 1.8],\n",
       "        [6.5, 3. , 5.8, 2.2],\n",
       "        [7.6, 3. , 6.6, 2.1],\n",
       "        [4.9, 2.5, 4.5, 1.7],\n",
       "        [7.3, 2.9, 6.3, 1.8],\n",
       "        [6.7, 2.5, 5.8, 1.8],\n",
       "        [7.2, 3.6, 6.1, 2.5],\n",
       "        [6.5, 3.2, 5.1, 2. ],\n",
       "        [6.4, 2.7, 5.3, 1.9],\n",
       "        [6.8, 3. , 5.5, 2.1],\n",
       "        [5.7, 2.5, 5. , 2. ],\n",
       "        [5.8, 2.8, 5.1, 2.4],\n",
       "        [6.4, 3.2, 5.3, 2.3],\n",
       "        [6.5, 3. , 5.5, 1.8],\n",
       "        [7.7, 3.8, 6.7, 2.2],\n",
       "        [7.7, 2.6, 6.9, 2.3],\n",
       "        [6. , 2.2, 5. , 1.5],\n",
       "        [6.9, 3.2, 5.7, 2.3],\n",
       "        [5.6, 2.8, 4.9, 2. ],\n",
       "        [7.7, 2.8, 6.7, 2. ],\n",
       "        [6.3, 2.7, 4.9, 1.8],\n",
       "        [6.7, 3.3, 5.7, 2.1],\n",
       "        [7.2, 3.2, 6. , 1.8],\n",
       "        [6.2, 2.8, 4.8, 1.8],\n",
       "        [6.1, 3. , 4.9, 1.8],\n",
       "        [6.4, 2.8, 5.6, 2.1],\n",
       "        [7.2, 3. , 5.8, 1.6],\n",
       "        [7.4, 2.8, 6.1, 1.9],\n",
       "        [7.9, 3.8, 6.4, 2. ],\n",
       "        [6.4, 2.8, 5.6, 2.2],\n",
       "        [6.3, 2.8, 5.1, 1.5],\n",
       "        [6.1, 2.6, 5.6, 1.4],\n",
       "        [7.7, 3. , 6.1, 2.3],\n",
       "        [6.3, 3.4, 5.6, 2.4],\n",
       "        [6.4, 3.1, 5.5, 1.8],\n",
       "        [6. , 3. , 4.8, 1.8],\n",
       "        [6.9, 3.1, 5.4, 2.1],\n",
       "        [6.7, 3.1, 5.6, 2.4],\n",
       "        [6.9, 3.1, 5.1, 2.3],\n",
       "        [5.8, 2.7, 5.1, 1.9],\n",
       "        [6.8, 3.2, 5.9, 2.3],\n",
       "        [6.7, 3.3, 5.7, 2.5],\n",
       "        [6.7, 3. , 5.2, 2.3],\n",
       "        [6.3, 2.5, 5. , 1.9],\n",
       "        [6.5, 3. , 5.2, 2. ],\n",
       "        [6.2, 3.4, 5.4, 2.3],\n",
       "        [5.9, 3. , 5.1, 1.8]]),\n",
       " 'target': array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "        1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2]),\n",
       " 'target_names': array(['setosa', 'versicolor', 'virginica'], dtype='<U10'),\n",
       " 'DESCR': '.. _iris_dataset:\\n\\nIris plants dataset\\n--------------------\\n\\n**Data Set Characteristics:**\\n\\n    :Number of Instances: 150 (50 in each of three classes)\\n    :Number of Attributes: 4 numeric, predictive attributes and the class\\n    :Attribute Information:\\n        - sepal length in cm\\n        - sepal width in cm\\n        - petal length in cm\\n        - petal width in cm\\n        - class:\\n                - Iris-Setosa\\n                - Iris-Versicolour\\n                - Iris-Virginica\\n                \\n    :Summary Statistics:\\n\\n    ============== ==== ==== ======= ===== ====================\\n                    Min  Max   Mean    SD   Class Correlation\\n    ============== ==== ==== ======= ===== ====================\\n    sepal length:   4.3  7.9   5.84   0.83    0.7826\\n    sepal width:    2.0  4.4   3.05   0.43   -0.4194\\n    petal length:   1.0  6.9   3.76   1.76    0.9490  (high!)\\n    petal width:    0.1  2.5   1.20   0.76    0.9565  (high!)\\n    ============== ==== ==== ======= ===== ====================\\n\\n    :Missing Attribute Values: None\\n    :Class Distribution: 33.3% for each of 3 classes.\\n    :Creator: R.A. Fisher\\n    :Donor: Michael Marshall (MARSHALL%PLU@io.arc.nasa.gov)\\n    :Date: July, 1988\\n\\nThe famous Iris database, first used by Sir R.A. Fisher. The dataset is taken\\nfrom Fisher\\'s paper. Note that it\\'s the same as in R, but not as in the UCI\\nMachine Learning Repository, which has two wrong data points.\\n\\nThis is perhaps the best known database to be found in the\\npattern recognition literature.  Fisher\\'s paper is a classic in the field and\\nis referenced frequently to this day.  (See Duda & Hart, for example.)  The\\ndata set contains 3 classes of 50 instances each, where each class refers to a\\ntype of iris plant.  One class is linearly separable from the other 2; the\\nlatter are NOT linearly separable from each other.\\n\\n.. topic:: References\\n\\n   - Fisher, R.A. \"The use of multiple measurements in taxonomic problems\"\\n     Annual Eugenics, 7, Part II, 179-188 (1936); also in \"Contributions to\\n     Mathematical Statistics\" (John Wiley, NY, 1950).\\n   - Duda, R.O., & Hart, P.E. (1973) Pattern Classification and Scene Analysis.\\n     (Q327.D83) John Wiley & Sons.  ISBN 0-471-22361-1.  See page 218.\\n   - Dasarathy, B.V. (1980) \"Nosing Around the Neighborhood: A New System\\n     Structure and Classification Rule for Recognition in Partially Exposed\\n     Environments\".  IEEE Transactions on Pattern Analysis and Machine\\n     Intelligence, Vol. PAMI-2, No. 1, 67-71.\\n   - Gates, G.W. (1972) \"The Reduced Nearest Neighbor Rule\".  IEEE Transactions\\n     on Information Theory, May 1972, 431-433.\\n   - See also: 1988 MLC Proceedings, 54-64.  Cheeseman et al\"s AUTOCLASS II\\n     conceptual clustering system finds 3 classes in the data.\\n   - Many, many more ...',\n",
       " 'feature_names': ['sepal length (cm)',\n",
       "  'sepal width (cm)',\n",
       "  'petal length (cm)',\n",
       "  'petal width (cm)'],\n",
       " 'filename': '/Users/zoedaron/anaconda3/lib/python3.7/site-packages/sklearn/datasets/data/iris.csv'}"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#what is this? #how did i get the attribute .target_names above?\n",
    "data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Exploration\n",
    "---\n",
    "It's always a good idea to get to know your data. \n",
    "1. What is the dimension of your data? I.e., how many rows and columns does it have? \n",
    "2. What does each row represent? \n",
    "3. What does each column represent?\n",
    "4. Each column represents a **feature**. What do these features mean? (hint: check our [syllabus](https://github.com/deerow22/STELLAR_interns/blob/master/Syllabus/Week_20.md))\n",
    "5. How are different features related?\n",
    "6. Do we have equal representation from each Iris type, or is there a bias? (hint: check [here](https://en.wikipedia.org/wiki/Iris_flower_data_set)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
      "0                5.1               3.5                1.4               0.2\n",
      "1                4.9               3.0                1.4               0.2\n",
      "2                4.7               3.2                1.3               0.2\n",
      "3                4.6               3.1                1.5               0.2\n",
      "4                5.0               3.6                1.4               0.2\n",
      "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
      "count         150.000000        150.000000         150.000000   \n",
      "mean            5.843333          3.057333           3.758000   \n",
      "std             0.828066          0.435866           1.765298   \n",
      "min             4.300000          2.000000           1.000000   \n",
      "25%             5.100000          2.800000           1.600000   \n",
      "50%             5.800000          3.000000           4.350000   \n",
      "75%             6.400000          3.300000           5.100000   \n",
      "max             7.900000          4.400000           6.900000   \n",
      "\n",
      "       petal width (cm)  \n",
      "count        150.000000  \n",
      "mean           1.199333  \n",
      "std            0.762238  \n",
      "min            0.100000  \n",
      "25%            0.300000  \n",
      "50%            1.300000  \n",
      "75%            1.800000  \n",
      "max            2.500000  \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "#put data into pandas for ease of working with\n",
    "iris = pd.DataFrame(data['data'], columns=data['feature_names'])\n",
    "\n",
    "print(iris.head()) #returns the first 5 rows of df\n",
    "print(iris.describe()) #summary statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "      <td>150.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>5.843333</td>\n",
       "      <td>3.057333</td>\n",
       "      <td>3.758000</td>\n",
       "      <td>1.199333</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.828066</td>\n",
       "      <td>0.435866</td>\n",
       "      <td>1.765298</td>\n",
       "      <td>0.762238</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>4.300000</td>\n",
       "      <td>2.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.100000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>5.100000</td>\n",
       "      <td>2.800000</td>\n",
       "      <td>1.600000</td>\n",
       "      <td>0.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>5.800000</td>\n",
       "      <td>3.000000</td>\n",
       "      <td>4.350000</td>\n",
       "      <td>1.300000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>6.400000</td>\n",
       "      <td>3.300000</td>\n",
       "      <td>5.100000</td>\n",
       "      <td>1.800000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>7.900000</td>\n",
       "      <td>4.400000</td>\n",
       "      <td>6.900000</td>\n",
       "      <td>2.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       sepal length (cm)  sepal width (cm)  petal length (cm)  \\\n",
       "count         150.000000        150.000000         150.000000   \n",
       "mean            5.843333          3.057333           3.758000   \n",
       "std             0.828066          0.435866           1.765298   \n",
       "min             4.300000          2.000000           1.000000   \n",
       "25%             5.100000          2.800000           1.600000   \n",
       "50%             5.800000          3.000000           4.350000   \n",
       "75%             6.400000          3.300000           5.100000   \n",
       "max             7.900000          4.400000           6.900000   \n",
       "\n",
       "       petal width (cm)  \n",
       "count        150.000000  \n",
       "mean           1.199333  \n",
       "std            0.762238  \n",
       "min            0.100000  \n",
       "25%            0.300000  \n",
       "50%            1.300000  \n",
       "75%            1.800000  \n",
       "max            2.500000  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#another way to visualize\n",
    "from IPython.display import display\n",
    "#if above gives errors open terminal and type: pip install IPython\n",
    "\n",
    "display(iris.head())\n",
    "display(iris.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>4.4</td>\n",
       "      <td>2.9</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>4.3</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.1</td>\n",
       "      <td>0.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>5.8</td>\n",
       "      <td>4.0</td>\n",
       "      <td>1.2</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>5.7</td>\n",
       "      <td>4.4</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.9</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>5.7</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.8</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>5.4</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.7</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.3</td>\n",
       "      <td>1.7</td>\n",
       "      <td>0.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>4.8</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.9</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>5.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.6</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>120</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>121</th>\n",
       "      <td>5.6</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.9</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>122</th>\n",
       "      <td>7.7</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.7</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.7</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>125</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.2</td>\n",
       "      <td>6.0</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>126</th>\n",
       "      <td>6.2</td>\n",
       "      <td>2.8</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>127</th>\n",
       "      <td>6.1</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.9</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>128</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>129</th>\n",
       "      <td>7.2</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.8</td>\n",
       "      <td>1.6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>130</th>\n",
       "      <td>7.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>6.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>131</th>\n",
       "      <td>7.9</td>\n",
       "      <td>3.8</td>\n",
       "      <td>6.4</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>132</th>\n",
       "      <td>6.4</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>133</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.8</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>134</th>\n",
       "      <td>6.1</td>\n",
       "      <td>2.6</td>\n",
       "      <td>5.6</td>\n",
       "      <td>1.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>135</th>\n",
       "      <td>7.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>136</th>\n",
       "      <td>6.3</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>137</th>\n",
       "      <td>6.4</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.5</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>138</th>\n",
       "      <td>6.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>4.8</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>139</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.6</td>\n",
       "      <td>2.4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141</th>\n",
       "      <td>6.9</td>\n",
       "      <td>3.1</td>\n",
       "      <td>5.1</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>142</th>\n",
       "      <td>5.8</td>\n",
       "      <td>2.7</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>143</th>\n",
       "      <td>6.8</td>\n",
       "      <td>3.2</td>\n",
       "      <td>5.9</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>144</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.3</td>\n",
       "      <td>5.7</td>\n",
       "      <td>2.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>145</th>\n",
       "      <td>6.7</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>146</th>\n",
       "      <td>6.3</td>\n",
       "      <td>2.5</td>\n",
       "      <td>5.0</td>\n",
       "      <td>1.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>6.5</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.2</td>\n",
       "      <td>2.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>148</th>\n",
       "      <td>6.2</td>\n",
       "      <td>3.4</td>\n",
       "      <td>5.4</td>\n",
       "      <td>2.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>5.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>5.1</td>\n",
       "      <td>1.8</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>150 rows Ã— 4 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                  5.1               3.5                1.4               0.2\n",
       "1                  4.9               3.0                1.4               0.2\n",
       "2                  4.7               3.2                1.3               0.2\n",
       "3                  4.6               3.1                1.5               0.2\n",
       "4                  5.0               3.6                1.4               0.2\n",
       "5                  5.4               3.9                1.7               0.4\n",
       "6                  4.6               3.4                1.4               0.3\n",
       "7                  5.0               3.4                1.5               0.2\n",
       "8                  4.4               2.9                1.4               0.2\n",
       "9                  4.9               3.1                1.5               0.1\n",
       "10                 5.4               3.7                1.5               0.2\n",
       "11                 4.8               3.4                1.6               0.2\n",
       "12                 4.8               3.0                1.4               0.1\n",
       "13                 4.3               3.0                1.1               0.1\n",
       "14                 5.8               4.0                1.2               0.2\n",
       "15                 5.7               4.4                1.5               0.4\n",
       "16                 5.4               3.9                1.3               0.4\n",
       "17                 5.1               3.5                1.4               0.3\n",
       "18                 5.7               3.8                1.7               0.3\n",
       "19                 5.1               3.8                1.5               0.3\n",
       "20                 5.4               3.4                1.7               0.2\n",
       "21                 5.1               3.7                1.5               0.4\n",
       "22                 4.6               3.6                1.0               0.2\n",
       "23                 5.1               3.3                1.7               0.5\n",
       "24                 4.8               3.4                1.9               0.2\n",
       "25                 5.0               3.0                1.6               0.2\n",
       "26                 5.0               3.4                1.6               0.4\n",
       "27                 5.2               3.5                1.5               0.2\n",
       "28                 5.2               3.4                1.4               0.2\n",
       "29                 4.7               3.2                1.6               0.2\n",
       "..                 ...               ...                ...               ...\n",
       "120                6.9               3.2                5.7               2.3\n",
       "121                5.6               2.8                4.9               2.0\n",
       "122                7.7               2.8                6.7               2.0\n",
       "123                6.3               2.7                4.9               1.8\n",
       "124                6.7               3.3                5.7               2.1\n",
       "125                7.2               3.2                6.0               1.8\n",
       "126                6.2               2.8                4.8               1.8\n",
       "127                6.1               3.0                4.9               1.8\n",
       "128                6.4               2.8                5.6               2.1\n",
       "129                7.2               3.0                5.8               1.6\n",
       "130                7.4               2.8                6.1               1.9\n",
       "131                7.9               3.8                6.4               2.0\n",
       "132                6.4               2.8                5.6               2.2\n",
       "133                6.3               2.8                5.1               1.5\n",
       "134                6.1               2.6                5.6               1.4\n",
       "135                7.7               3.0                6.1               2.3\n",
       "136                6.3               3.4                5.6               2.4\n",
       "137                6.4               3.1                5.5               1.8\n",
       "138                6.0               3.0                4.8               1.8\n",
       "139                6.9               3.1                5.4               2.1\n",
       "140                6.7               3.1                5.6               2.4\n",
       "141                6.9               3.1                5.1               2.3\n",
       "142                5.8               2.7                5.1               1.9\n",
       "143                6.8               3.2                5.9               2.3\n",
       "144                6.7               3.3                5.7               2.5\n",
       "145                6.7               3.0                5.2               2.3\n",
       "146                6.3               2.5                5.0               1.9\n",
       "147                6.5               3.0                5.2               2.0\n",
       "148                6.2               3.4                5.4               2.3\n",
       "149                5.9               3.0                5.1               1.8\n",
       "\n",
       "[150 rows x 4 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#for question #1 you might want to write code in this cell\n",
    "iris"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<Figure size 1000x1000 with 16 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#visualizing as a corner plot\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "pd.plotting.scatter_matrix(iris, c=data['target'],marker='o', s=10,alpha=.8,figsize=(10,10))\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Correlation heatmap\n",
    "\n",
    "Correlation is a good way to see the relationships among different pairs of variables. Here we use the fuction inside a DataFrame and visualize the result as heatmap using Seaborn package. \n",
    "- The range for correlation is -1 to 1, where -1 is completely negatively correlated, 0 is no correlation, and +1 is completely positively correlated.\n",
    "\n",
    "_Want to know the type of correlation? Want to know what \".corr()\" means? Want to know what a heapmap is in more detail? Use your Google skills generously :)_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1a1f15deb8>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "corr = iris.corr()\n",
    "sns.heatmap(corr)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PAUSE \n",
    "## For [Slides](https://docs.google.com/presentation/d/1RiTfKWxTOJ_BGvKTSdcKBNndkEbiK9RznOXFUjCuG-A/edit?usp=sharing) \n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building a decision tree model\n",
    "---\n",
    "Scikit-learn Decision Trees\n",
    "\n",
    "Decision trees is a **supervised learning method** (meaning that there is a target feature we are trying to predict). It is a method used for classification (e.g., is a house above a certain value) and regression (e.g., how much does a house cost). The goal is to **create a model that predicts the value of a target feature by learning simple decision rules inferred from the other data features.**\n",
    "\n",
    "We will use a decision tree algorithm from the popular **scikit-learn** machine learning libray (http://scikit-learn.org/stable/) to build a decision tree model. The library contains efficient tools with relatively simple interface for data mining and data analysis, and reusable in various contexts. Scikit-learn and Pandas (and Numpy if you continue to dabble in machine learning) are your best friends.\n",
    "\n",
    "**DecisionTreeClassifier** is a class within scikit-learn that is capable of performing multi-class classification on a dataset. For example, it can classify a given data point of an Iris dataset into three different flowers. For more information: http://scikit-learn.org/stable/modules/tree.html\n",
    "\n",
    "DecisionTreeClassifier takes 2 inputs: \n",
    "- a matrix X, of size [n_samples, n_features] holding the **training samples**, and \n",
    "- an array Y of integer values, size [n_samples], holding the **class labels** for the training samples\n",
    "\n",
    "More documentation on DecisionTreeClassifier parameters can be found [here](http://scikit-learn.org/stable/modules/generated/sklearn.tree.DecisionTreeClassifier.html#sklearn.tree.DecisionTreeClassifier)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining inputs\n",
    "\n",
    "X = iris\n",
    "y = data.target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n",
      "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
      "            max_features=None, max_leaf_nodes=None,\n",
      "            min_impurity_decrease=0.0, min_impurity_split=None,\n",
      "            min_samples_leaf=1, min_samples_split=2,\n",
      "            min_weight_fraction_leaf=0.0, presort=False, random_state=None,\n",
      "            splitter='best')\n"
     ]
    }
   ],
   "source": [
    "## Import the decision tree classifier from library (DecisionTreeClassifier)\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "\n",
    "\n",
    "# Define your model here (one line of code)\n",
    "model = DecisionTreeClassifier()\n",
    "print(model)\n",
    "# Fit your model here (one line of code)\n",
    "fit_model = model.fit(X, y)\n",
    "print(fit_model)\n",
    "\n",
    "\n",
    "#note: from documentation, \"training input samples\" are the features you want to build your model with\n",
    "#and \"target values\" is the feature you want to predict \n",
    "\n",
    "#Hint: You do not need to code anything explicitly. Just use standard methods for this class.\n",
    "#If you feel lost at any point, refer the documentation which has examples for using different methods you will need. \n",
    "#i.e. BIG HINT: use the \"For more information:\" url link above"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using your model, predict the classifications for some flowers and reflect on those predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0\n",
      " 0 0 0 0 0 0 0 0 0 0 0 0 0 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1\n",
      " 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 1 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2 2\n",
      " 2 2]\n"
     ]
    }
   ],
   "source": [
    "#generate predictions from your model\n",
    "\n",
    "predicted = model.predict(X)\n",
    "print(predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [1. 0. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 1. 0.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]\n",
      " [0. 0. 1.]]\n"
     ]
    }
   ],
   "source": [
    "#generate prediction probabilities from your model\n",
    "\n",
    "probabilities = fit_model.predict_proba(X)\n",
    "print(probabilities)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reflection 1\n",
    "---\n",
    "\n",
    "How do you feel about these predictions? Now use the Iris data in the Test Datasets folder to predict a classification for a new data point? How do you feel about this prediction? Take a few minutes to think about these questions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   sepal_length  sepal_width  petal_length  petal_width\n",
      "0          5.95         3.10          2.45         1.75\n",
      "1          5.84         3.06          3.76         1.20\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "mydata= {'sepal_length':[5.95, 5.84] ,'sepal_width':[3.1,3.06] ,'petal_length':[2.45, 3.76] ,'petal_width':[1.75,1.2]}\n",
    "df = pd.DataFrame(mydata)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 1]\n"
     ]
    }
   ],
   "source": [
    "predicted2 = model.predict(df)\n",
    "print(predicted2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tree Visualization\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "module 'matplotlib.pyplot' has no attribute 'figfure'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-cecda6fd5e72>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mplt\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfigfure\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m15\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m \u001b[0mtree\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_tree\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m;\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'matplotlib.pyplot' has no attribute 'figfure'"
     ]
    }
   ],
   "source": [
    "from sklearn import tree\n",
    "plt.figfure(figsize = (20, 15))\n",
    "tree.plot_tree(model.fit(data.data, data.target)); "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Manually trace your new Iris datapoint through your decision tree. Did you get the same answer?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How good is your model?\n",
    "---\n",
    "There are various metrics to test how good a model is, and one is accuracy. Conveniently scikit-learn includes built in functionality for this, as well as other metrics you could choose.\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.metrics\n",
    "\n",
    "Note: this is a bit of a cheat because we are using the **same data** to both build the tree, and then running that data through the tree to predict values. How could you improve this? \n",
    "\n",
    "Since we used the same data for training and predicting, would you expect the accuracy to be high? What does it mean when the accuracy is not 100%?\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [150, 38]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f4fde5ed5dba>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmetrics\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0maccuracy_score\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredicted\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36maccuracy_score\u001b[0;34m(y_true, y_pred, normalize, sample_weight)\u001b[0m\n\u001b[1;32m    174\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    175\u001b[0m     \u001b[0;31m# Compute accuracy for each possible representation\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 176\u001b[0;31m     \u001b[0my_type\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_check_targets\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    177\u001b[0m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    178\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0my_type\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstartswith\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'multilabel'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/metrics/classification.py\u001b[0m in \u001b[0;36m_check_targets\u001b[0;34m(y_true, y_pred)\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0my_pred\u001b[0m \u001b[0;34m:\u001b[0m \u001b[0marray\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mindicator\u001b[0m \u001b[0mmatrix\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     70\u001b[0m     \"\"\"\n\u001b[0;32m---> 71\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     72\u001b[0m     \u001b[0mtype_true\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_true\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m     \u001b[0mtype_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtype_of_target\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0my_pred\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    233\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    234\u001b[0m         raise ValueError(\"Found input variables with inconsistent numbers of\"\n\u001b[0;32m--> 235\u001b[0;31m                          \" samples: %r\" % [int(l) for l in lengths])\n\u001b[0m\u001b[1;32m    236\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    237\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [150, 38]"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "accuracy_score(y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training and testing the accuracy of your model\n",
    "---\n",
    "### Training set and test set\n",
    "\n",
    "Above, we used the same data to build/train the model and calculate the accuracy of the model. In real life applications, say you had money in the game, why might that be bad? What might be a better way to test the accuracy of your model? \n",
    "\n",
    "Not use the same data to generate the model and then test the model accuracy.\n",
    "\n",
    "The practical value of a model comes from making predictions on new data, so we should measure performance on data that wasn't used to build the model. The most straightforward way to do this is to **exclude some data** from the model-building process, and then **use the excluded data to test the accuracy of the model**. This excluded data is called validation/testing data.\n",
    "\n",
    "We will split the dataset into a training set, to build the decision tree on, and a validation set. For the validation set, we pretend we don't know the price, predict the price using the decision tree built on the training set,  and then compare the predicted price to the true price to see how well we did.\n",
    "\n",
    "Splitting data sets into a training set and a validation set is a very common operation in machine learning, so scikit-learn has nice funcationality for this operation\n",
    "\n",
    "http://scikit-learn.org/stable/modules/classes.html#module-sklearn.model_selection\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421052631578947"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state = 11)\n",
    "\n",
    "\n",
    "#NOTICE: below are the same steps as before, except our data sample \n",
    "model = DecisionTreeClassifier()\n",
    "#fit the model to the training set\n",
    "fit_model = model.fit(train_X,train_y)\n",
    "#compare predictions from this model to the test set\n",
    "predicted = fit_model.predict(test_X)\n",
    "accuracy_score(test_y, predicted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rerun the cell above multiple times. What do you think is happening to the accuracy score?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.8421052631578947"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_X, test_X, train_y, test_y = train_test_split(X, y, random_state = 11)\n",
    "\n",
    "\n",
    "#NOTICE: below are the same steps as before, except our data sample \n",
    "model = DecisionTreeClassifier()\n",
    "#fit the model to the training set\n",
    "fit_model = model.fit(train_X,train_y)\n",
    "#compare predictions from this model to the test set\n",
    "predicted = fit_model.predict(test_X)\n",
    "accuracy_score(test_y, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
